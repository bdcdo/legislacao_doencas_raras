import requests
import re
import time
import pandas as pd
from datetime import datetime
from bs4 import BeautifulSoup as bs


def parse_article(article):
    """
    Extrai informa√ß√µes relevantes de um artigo do CFM
    """
    result = {}
    
    # Extrair informa√ß√µes do cabe√ßalho
    header = article.find('div', class_='card-header')
    if header:
        ul = header.find('ul')
        if ul:
            items = ul.find_all('li')
            for item in items:
                strong = item.find('strong')
                p = item.find('p')
                if strong and p:
                    key = strong.text.strip()
                    value = p.text.strip()
                    result[key] = value
    
    # Extrair ementa
    body = article.find('div', class_='card-body')
    if body:
        ementa_span = body.find('span')
        if ementa_span:
            result['Ementa'] = ementa_span.text.strip()
    
    # Extrair link para a norma
    link = body.find('a', class_='btn btn-primary') if body else None
    if link and link.get('href'):
        result['Link'] = link.get('href')
    
    return result


def extract_pagination_info(soup):
    """
    Extrai informa√ß√µes de pagina√ß√£o da p√°gina do CFM
    """
    pagination_info = {}
    
    # Buscar por links de pagina√ß√£o
    pagination_links = soup.find_all('a', class_='link-navigation')
    
    # Extrair n√∫meros de p√°gina
    page_numbers = []
    for link in pagination_links:
        text = link.text.strip()
        if text.isdigit():
            page_numbers.append(int(text))
    
    if page_numbers:
        pagination_info['total_pages'] = max(page_numbers)
    
    # Buscar informa√ß√µes textuais de p√°gina
    page_info_divs = soup.find_all('div', class_='pt-3')
    for div in page_info_divs:
        text = div.text.strip()
        if 'Mostrando p√°gina' in text and 'de' in text:
            # Extrair p√°gina atual e total
            match = re.search(r'Mostrando p√°gina (\d+) de (\d+)', text)
            if match:
                pagination_info['current_page'] = int(match.group(1))
                pagination_info['total_pages'] = int(match.group(2))
    
    # Buscar total de registros
    all_text = soup.get_text()
    records_match = re.search(r'(\d+)\s+registros encontrados', all_text)
    if records_match:
        pagination_info['total_records'] = int(records_match.group(1))
    
    return pagination_info


def extract_all_articles(html_content):
    """
    Extrai informa√ß√µes de todos os artigos de uma p√°gina do CFM
    """
    soup = bs(html_content, 'html.parser')
    results_div = soup.find('div', attrs={'id':'resultsNormas'})
    
    if not results_div:
        return []
    
    articles = results_div.find_all('article')
    parsed_articles = []
    
    for article in articles:
        parsed_article = parse_article(article)
        if parsed_article:  # S√≥ adiciona se conseguiu extrair algo
            parsed_articles.append(parsed_article)
    
    return parsed_articles


def search_cfm_norms(search_term="doen√ßas raras", page=1):
    """
    Busca normas do CFM por termo de pesquisa
    """
    url = "https://portal.cfm.org.br/buscar-normas-cfm-e-crm/#resultado"
    
    querystring = {
        "tipo[0]": "R",
        "tipo[1]": "P", 
        "tipo[2]": "E",
        "tipo[3]": "N",
        "tipo[4]": "D",
        "uf": "",
        "revogada": "",
        "numero": "",
        "ano": "",
        "ta": "OU",
        "assunto[0]": "",
        "texto": search_term,
        "pagina": str(page)
    }
    
    headers = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "pt-BR,en-US;q=0.7,en;q=0.3",
        "Connection": "keep-alive",
        "DNT": "1",
        "Priority": "u=0, i",
        "Referer": "https://portal.cfm.org.br/buscar-normas-cfm-e-crm",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-User": "?1",
        "Sec-GPC": "1",
        "Upgrade-Insecure-Requests": "1",
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:141.0) Gecko/20100101 Firefox/141.0"
    }
    
    try:
        response = requests.get(url, headers=headers, params=querystring)
        response.raise_for_status()
        return response
    except requests.RequestException as e:
        print(f"Erro na requisi√ß√£o: {e}")
        return None


def search_all_terms():
    """
    Busca normas do CFM para todos os termos relacionados a doen√ßas raras
    """
    search_terms = [
        'doen√ßa rara',
        'doen√ßa ultrarrara', 
        'doen√ßas raras',
        'doen√ßas ultrarraras',
        'medicamento √≥rf√£o',
        'medicamentos √≥rf√£os',
        'patologia rara',
        'patologia ultrarrara',
        'patologias raras',
        'patologias ultrarraras',
        's√≠ndrome rara',
        's√≠ndrome ultrarrara',
        's√≠ndromes raras',
        's√≠ndromes ultrarraras',
        'terapia √≥rf√£',
        'terapias √≥rf√£s'
    ]
    
    all_results = []
    
    for term in search_terms:
        print(f"\n{'='*60}")
        print(f"Buscando termo: '{term}'")
        print(f"{'='*60}")
        
        # Fazer requisi√ß√£o para primeira p√°gina
        response = search_cfm_norms(term, page=1)
        
        if not response:
            print(f"Falha na requisi√ß√£o para termo '{term}'")
            continue
        
        # Parse do HTML
        soup = bs(response.content, 'html.parser')
        
        # Extrair informa√ß√µes de pagina√ß√£o
        pagination_info = extract_pagination_info(soup)
        
        print(f"Termo: {term}")
        if pagination_info:
            print(f"Total de registros: {pagination_info.get('total_records', 'N/A')}")
            print(f"Total de p√°ginas: {pagination_info.get('total_pages', 'N/A')}")
        
        # Se n√£o h√° registros, continua para pr√≥ximo termo
        if not pagination_info.get('total_records', 0):
            print("Nenhum registro encontrado")
            continue
            
        # Extrair artigos de todas as p√°ginas
        term_articles = []
        total_pages = pagination_info.get('total_pages', 1)
        
        for page in range(1, total_pages + 1):
            print(f"Processando p√°gina {page}/{total_pages}...")
            
            if page > 1:  # J√° fizemos a primeira p√°gina
                response = search_cfm_norms(term, page=page)
                if not response:
                    print(f"Falha na requisi√ß√£o p√°gina {page}")
                    continue
            
            # Extrair artigos da p√°gina atual
            page_articles = extract_all_articles(response.content)
            
            # Adicionar termo de busca aos artigos
            for article in page_articles:
                article['termo_busca'] = term
                article['pagina'] = page
            
            term_articles.extend(page_articles)
            
            # Delay entre requisi√ß√µes para n√£o sobrecarregar o servidor
            if page < total_pages:
                time.sleep(1)
        
        print(f"Total de artigos coletados para '{term}': {len(term_articles)}")
        all_results.extend(term_articles)
        
        # Delay entre termos de busca
        time.sleep(2)
    
    return all_results


def save_to_csv(articles, filename=None):
    """
    Salva os artigos coletados em um arquivo CSV
    """
    if not articles:
        print("Nenhum artigo para salvar")
        return None
    
    # Gerar nome do arquivo se n√£o fornecido
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"cfm_normas_doencas_raras_{timestamp}.csv"
    
    # Converter para DataFrame
    df = pd.DataFrame(articles)
    
    # Reordenar colunas para melhor visualiza√ß√£o
    column_order = ['termo_busca', 'Tipo', 'UF', 'N¬∫/Ano', 'Situa√ß√£o', 'Ementa', 'Link', 'pagina']
    
    # Manter apenas colunas que existem
    existing_columns = [col for col in column_order if col in df.columns]
    remaining_columns = [col for col in df.columns if col not in column_order]
    final_columns = existing_columns + remaining_columns
    
    df = df[final_columns]
    
    # Salvar CSV
    try:
        df.to_csv(filename, index=False, encoding='utf-8')
        print(f"\n‚úÖ Dados salvos em: {filename}")
        print(f"üìä Total de registros: {len(df)}")
        print(f"üìã Colunas: {list(df.columns)}")
        return filename
    except Exception as e:
        print(f"‚ùå Erro ao salvar CSV: {e}")
        return None


def generate_summary_report(articles):
    """
    Gera relat√≥rio resumido dos resultados
    """
    if not articles:
        return
    
    from collections import defaultdict, Counter
    
    # Estat√≠sticas gerais
    print(f"\n{'='*60}")
    print(f"RELAT√ìRIO RESUMIDO")
    print(f"{'='*60}")
    print(f"Total de artigos coletados: {len(articles)}")
    
    # Distribui√ß√£o por termo de busca
    articles_by_term = defaultdict(list)
    for article in articles:
        term = article.get('termo_busca', 'N/A')
        articles_by_term[term].append(article)
    
    print("\nüìà Distribui√ß√£o por termo de busca:")
    for term, term_articles in sorted(articles_by_term.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"  ‚Ä¢ {term}: {len(term_articles)} artigos")
    
    # Distribui√ß√£o por tipo
    tipos = [article.get('Tipo', 'N/A') for article in articles]
    tipo_counts = Counter(tipos)
    
    print("\nüìä Distribui√ß√£o por tipo de norma:")
    for tipo, count in tipo_counts.most_common():
        print(f"  ‚Ä¢ {tipo}: {count} artigos")
    
    # Distribui√ß√£o por UF
    ufs = [article.get('UF', 'N/A') for article in articles]
    uf_counts = Counter(ufs)
    
    print("\nüó∫Ô∏è  Distribui√ß√£o por UF:")
    for uf, count in uf_counts.most_common():
        print(f"  ‚Ä¢ {uf}: {count} artigos")
    
    # Distribui√ß√£o por situa√ß√£o
    situacoes = [article.get('Situa√ß√£o', 'N/A') for article in articles]
    situacao_counts = Counter(situacoes)
    
    print("\n‚öñÔ∏è  Distribui√ß√£o por situa√ß√£o:")
    for situacao, count in situacao_counts.most_common():
        print(f"  ‚Ä¢ {situacao}: {count} artigos")


def main():
    """
    Fun√ß√£o principal para executar a busca e parsing
    """
    print("Iniciando busca completa de normas do CFM sobre doen√ßas raras...")
    
    # Buscar todos os termos
    all_articles = search_all_terms()
    
    if not all_articles:
        print("‚ùå Nenhum artigo foi coletado")
        return
    
    # Gerar relat√≥rio resumido
    generate_summary_report(all_articles)
    
    # Salvar em CSV
    csv_filename = save_to_csv(all_articles)
    
    # Mostrar alguns exemplos
    print(f"\n{'='*60}")
    print("PRIMEIROS 3 ARTIGOS:")
    print(f"{'='*60}")
    
    for i, article in enumerate(all_articles[:3], 1):
        print(f"\nARTIGO {i} (Termo: {article.get('termo_busca', 'N/A')}):")
        print("-" * 50)
        for key, value in article.items():
            if key in ['termo_busca', 'pagina']:
                continue
            if key == 'Ementa':
                ementa_preview = value[:100] + "..." if len(value) > 100 else value
                print(f"{key}: {ementa_preview}")
            else:
                print(f"{key}: {value}")
    
    if csv_filename:
        print(f"\nüéØ Processo conclu√≠do! Dados salvos em: {csv_filename}")


if __name__ == "__main__":
    main()