{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d328e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(response, infos):\n",
    "    # 2. PARSE THE XML RESPONSE (Our Previous Logic)\n",
    "    # ================================================\n",
    "    try:\n",
    "        # The response.text is the XML data\n",
    "        xml_data = response.text\n",
    "        \n",
    "        # Parse the entire document as XML\n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        # Find the <update id=\"form\"> tag and get its text content\n",
    "        # This gives us the clean HTML string\n",
    "        html_string = root.find('.//update[@id=\"form\"]').text\n",
    "\n",
    "        # Parse the extracted HTML string with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "        # Find the results table (the colon ':' must be escaped with a backslash)\n",
    "        results_table = soup.find('table', id='form:grid')\n",
    "        \n",
    "        if not results_table:\n",
    "            print(\"❌ Could not find the results table in the response.\")\n",
    "        else:\n",
    "            # Find all table rows in the tbody\n",
    "            rows = results_table.find('tbody').find_all('tr')\n",
    "            print(f\"\\nFound {len(rows)} results on this page.\\n\")\n",
    "\n",
    "            # 3. EXTRACT AND DISPLAY DATA FROM THE TABLE\n",
    "            # =============================================\n",
    "            for i, row in enumerate(rows):\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) < 8: # Ensure the row has enough columns\n",
    "                    continue\n",
    "                \n",
    "                num_ident = cells[0].text.strip()\n",
    "                numero = cells[1].text.strip()\n",
    "                origem = cells[2].text.strip()\n",
    "                tipo_norma = cells[3].text.strip()\n",
    "                data_pub = cells[4].text.strip()\n",
    "                ementa = cells[5].text.strip()\n",
    "                link_tag = cells[7].find('a', {'title': 'Texto Completo'})\n",
    "                link_url = link_tag['href'] if link_tag else 'N/A'\n",
    "\n",
    "                infos.append([tipo_norma, numero, data_pub, origem, ementa, link_url])\n",
    "\n",
    "    except ET.ParseError:\n",
    "        print(\"❌ Failed to parse XML. The response might not be in the expected format.\")\n",
    "        print(\"Response Text:\\n\", response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    \n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0bf928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 HTML files to process:\n",
      "  - search_results_page_1_20250724_230036.html\n",
      "  - search_results_initial_20250724_230036.html\n",
      "  - search_results_page_2_20250724_230036.html\n",
      "  - search_results_initial_20250724_230308.html\n",
      "  - search_results_page_3_20250724_230036.html\n",
      "\n",
      "Processing: search_results_page_1_20250724_230036.html\n",
      "Found 10 results in this file\n",
      "\n",
      "Processing: search_results_initial_20250724_230036.html\n",
      "Found 10 results in this file\n",
      "\n",
      "Processing: search_results_page_2_20250724_230036.html\n",
      "Found 10 results in this file\n",
      "\n",
      "Processing: search_results_initial_20250724_230308.html\n",
      "Found 9 results in this file\n",
      "\n",
      "Processing: search_results_page_3_20250724_230036.html\n",
      "Found 10 results in this file\n",
      "\n",
      "✅ Processing complete! Total records collected: 49\n",
      "Sample records:\n",
      "  1. PRT - 514 (28/04/2023)\n",
      "  2. PRT - 1240 (13/09/2023)\n",
      "  3. PRT - 887 (18/07/2023)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get all HTML files from the saudelegis_pages directory\n",
    "html_files = glob.glob('/home/bdcdo/Desktop/dev/sabara-doencas-raras/v3-consolidacao/saudelegis_pages/*.html')\n",
    "print(f\"Found {len(html_files)} HTML files to process:\")\n",
    "for file in html_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Initialize list to store all parsed information\n",
    "all_infos = []\n",
    "\n",
    "# Process each HTML file\n",
    "for html_file in html_files:\n",
    "    print(f\"\\nProcessing: {os.path.basename(html_file)}\")\n",
    "    \n",
    "    # Read the HTML file\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the results table\n",
    "    results_table = soup.find('table', id='form:grid')\n",
    "    \n",
    "    if not results_table:\n",
    "        print(f\"❌ Could not find the results table in {os.path.basename(html_file)}\")\n",
    "        continue\n",
    "    \n",
    "    # Find all table rows in the tbody\n",
    "    tbody = results_table.find('tbody')\n",
    "    if not tbody:\n",
    "        print(f\"❌ Could not find tbody in {os.path.basename(html_file)}\")\n",
    "        continue\n",
    "        \n",
    "    rows = tbody.find_all('tr')\n",
    "    print(f\"Found {len(rows)} results in this file\")\n",
    "    \n",
    "    # Extract data from each row\n",
    "    for i, row in enumerate(rows):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) < 8:  # Ensure the row has enough columns\n",
    "            continue\n",
    "        \n",
    "        num_ident = cells[0].text.strip()\n",
    "        numero = cells[1].text.strip()\n",
    "        origem = cells[2].text.strip()\n",
    "        tipo_norma = cells[3].text.strip()\n",
    "        data_pub = cells[4].text.strip()\n",
    "        ementa = cells[5].text.strip()\n",
    "        link_tag = cells[7].find('a', {'title': 'Texto Completo'})\n",
    "        link_url = link_tag['href'] if link_tag else 'N/A'\n",
    "        \n",
    "        all_infos.append([tipo_norma, numero, data_pub, origem, ementa, link_url])\n",
    "\n",
    "print(f\"\\n✅ Processing complete! Total records collected: {len(all_infos)}\")\n",
    "print(f\"Sample records:\")\n",
    "for i, info in enumerate(all_infos[:3]):  # Show first 3 records\n",
    "    print(f\"  {i+1}. {info[0]} - {info[1]} ({info[2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816d0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_infos, columns=['tipo_norma', 'numero', 'data_pub', 'origem', 'ementa', 'link_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87fb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ms.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabara-doencas-raras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
